w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- 1
while(sqrt(sum((w_new-w)^2)) > tol & iter <= maxiter){
w <- w_new
w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- iter + 1
}
return(list(root = w_new, gradient_value = gradient(w_new, mu, X), converged = iter <= maxiter, iterations = iter))
}
NR_algorithm(w_init = numeric(length = 9), mu, X, gradient, hessian)
w <- numeric(length = length(X[1,]))
#functions to calculate p, a, b as on page 1302
p <- function(w, X){
return(sigmoid(X %*% w))
}
p(w, X)
a <- function(alpha, Ann){
alpha ^(Ann)
}
a(alpha, Ann)
a <- function(alpha, Ann){
t(alpha) ^ (Ann)
}
a(alpha, Ann)
a <- function(alpha, Ann){
alpha ^ t(Ann)
}
a(alpha, Ann)
View(Ann)
a <- function(alpha, Ann){
alpha ^ as.vector(Ann)
}
a(alpha, Ann)
a <- function(alpha, Ann){
matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
}
a(alpha, Ann)
a <- function(alpha, Ann){
matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
matrix((1-alpha) ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
}
a <- function(alpha, Ann){
matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
matrix((1-alpha) ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
}
a(alpha, Ann)
install.packages("Rfast")
##Implementation of Raykar-Algorithm for Binary Classification
library(Rfast)
a <- function(alpha, Ann){
alpha_powers <- matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
one_minus_alpha_powers <- matrix((1-alpha) ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
all_powers <- alpha_powers * one_minus_alpha_powers
return(colprods(all_powers))
}
a(alpha, Ann)
##Implementation of Raykar-Algorithm for Binary Classification
library(terrUtils)
install.packages("terrUtils")
a <- function(alpha, Ann){
alpha_powers <- matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
one_minus_alpha_powers <- matrix((1-alpha) ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
all_powers <- alpha_powers * one_minus_alpha_powers
return(apply(all_powers,1,prod))
}
a(alpha, Ann)
#functions to calculate p, a, b as on page 1302
p <- function(w, X){
return(sigmoid(X %*% w))
}
a <- function(alpha, Ann){
alpha_powers <- matrix(alpha ^ as.vector(Ann), ncol = 3, byrow = TRUE)
one_minus_alpha_powers <- matrix((1-alpha) ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
all_powers <- alpha_powers * one_minus_alpha_powers
return(apply(all_powers,1,prod))
}
b <- function(beta, Ann){
alpha_powers <- matrix(beta ^ as.vector(1-Ann), ncol = 3, byrow = TRUE)
one_minus_alpha_powers <- matrix((1-beta) ^ as.vector(Ann), ncol = 3, byrow = TRUE)
all_powers <- alpha_powers * one_minus_alpha_powers
return(apply(all_powers,1,prod))
}
a(alpha, Ann)
b(beta, Ann)
theta <- c(alpha, beta, w)
#EM-algorithm loop
mu_new <- a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X)))
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
w <- as.vector(NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root)
theta <- c(alpha, beta, w)
#set tolerance
tol <- 10^-4
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance
tol <- 10^-4
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
N <- length(data$Diagnosis)
R <- 3
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-4
max_iter <- 10^5
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(c(iter, sqrt(sum((theta_new - theta)^2))))
}
#set tolerance, max_iter and counter
tol <- 10^-5
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(c(iter, sqrt(sum((theta_new - theta)^2))))
}
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
N <- length(data$Diagnosis)
R <- 3
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-5
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(c(iter, sqrt(sum((theta_new - theta)^2))))
}
#Newton-Raphson algorithm to approximate root of gradient of (3) wrt w (page 1303)
NR_algorithm <- function(w_init, mu, X, gradient, hessian, eta = 0.7, tol = .Machine$double.eps, maxiter = 10^6){
w <- w_init
w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- 1
while(sqrt(sum((w_new-w)^2)) > tol & iter <= maxiter){
w <- w_new
w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- iter + 1
}
return(list(root = as.vector(w_new), gradient_value = gradient(w_new, mu, X), converged = iter <= maxiter, iterations = iter))
}
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-5
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(c(iter, sqrt(sum((theta_new - theta)^2))))
}
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
N <- length(data$Diagnosis)
R <- 3
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-5
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
NR <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)
w_new <- NR$root
print(NR$iterations)
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(c(iter, sqrt(sum((theta_new - theta)^2))))
}
#set tolerance, max_iter and counter
tol <- 10^-6
#set tolerance, max_iter and counter
tol <- 10^-6
max_iter <- 100
iter <- 1
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
NR <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)
w_new <- NR$root
print(NR$iterations)
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(sqrt(sum((theta_new - theta)^2)))
}
#Newton-Raphson algorithm to approximate root of gradient of (3) wrt w (page 1303)
NR_algorithm <- function(w_init, mu, X, gradient, hessian, eta = 0.6, tol = .Machine$double.eps, maxiter = 10^3){
w <- w_init
w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- 1
while(sqrt(sum((w_new-w)^2)) > tol & iter <= maxiter){
w <- w_new
w_new <- w - eta * solve(hessian(w, X)) %*% gradient(w, mu, X)
iter <- iter + 1
}
return(list(root = as.vector(w_new), gradient_value = gradient(w_new, mu, X), converged = iter <= maxiter, iterations = iter))
}
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
N <- length(data$Diagnosis)
R <- 3
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-6
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
NR <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)
w_new <- NR$root
print(NR$iterations)
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(sqrt(sum((theta_new - theta)^2)))
}
theta_new
##Implementation of Raykar-Algorithm for Binary Classification
source("raykar_binary_classification_functions.r")
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
N <- length(data$Diagnosis)
R <- 3
#get matrix of annotators (Ann) & predictors (X)
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
X <- data[, -c(1,11,12,13,14)] #remove response variables
X <- as.matrix(X)
colnames(X) <- NULL
#initialize mu with majority vote (as commented on page 1303)
mu <- colSums(Ann) / R
#initialize sensitivities (alpha), specificities (beta) for annotators, weight vector (w) as on page 1303
alpha <- as.vector(Ann %*% mu / sum(mu))
beta <- as.vector((1 - Ann) %*% (1 - mu) / sum(1 - mu))
w <- NR_algorithm(w_init = numeric(length = length(X[1,])), mu, X, gradient, hessian)$root
theta <- c(alpha, beta, w)
#set tolerance, max_iter and counter
tol <- 10^-6
max_iter <- 100
iter <- 1
#EM-algorithm loop
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
w_new <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)$root
theta_new <- c(alpha_new, beta_new, w_new)
while(sqrt(sum((theta_new - theta)^2)) > tol & iter <= max_iter){
alpha <- alpha_new
beta <- beta_new
w <- w_new
theta <- theta_new
mu_new <- as.vector(a(alpha, Ann) * p(w, X) / (a(alpha, Ann) * p(w, X) + b(beta, Ann) *  (1 - p(w, X))))
alpha_new <- as.vector(Ann %*% mu_new / sum(mu_new))
beta_new <- as.vector((1 - Ann) %*% (1 - mu_new) / sum(1 - mu_new))
NR <- NR_algorithm(w_init = w, mu_new, X, gradient, hessian)
w_new <- NR$root
print(NR$iterations)
theta_new <- c(alpha_new, beta_new, w_new)
iter <- iter + 1
print(sqrt(sum((theta_new - theta)^2)))
}
source('E:/Studium/09_WS2122/SML_Project/data_screening.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
data$Diagnosis
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
data_ground_truth <- data[,-c(1, 12, 13, 14)]
model_ground_truth <- glm(Diagnosis ~ ., family = binomial, data = data_ground_truth)
model_ground_truth$fitted.values
as.numeric(model_ground_truth$fitted.values > 0.5)
colSums(Ann)
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
colSums(Ann)
R <- 3
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
colSums(Ann) / R
Diagnosis_majority <- as.numeric(colSums(Ann) / R > 0.5)
data_majority <- data[,-c(1, 11, 12, 13, 14)]
data_majority$DiagnosisMajority <- Diagnosis_majority
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
Diagnosis_majority <- as.numeric((colSums(Ann)) / R > 0.5)
data_majority <- data[,-c(1, 11, 12, 13, 14)]
data_majority$DiagnosisMajority <- Diagnosis_majority
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
Diagnosis_majority <- as.numeric((colSums(Ann)) / R > 0.5)
data_majority <- data[,-c(1, 11, 12, 13, 14)]
data_majority$DiagnosisMajority <- Diagnosis_majority
model_majority <- glm(DiagnosisMajority ~ ., family = binomial, data = data_majority)
as.numeric(model_majority$fitted.values > 0.5)
data$Diagnosis
as.numeric(model_ground_truth$fitted.values > 0.5)
as.numeric(model_ground_truth$fitted.values > 0.5)
as.numeric(model_majority$fitted.values > 0.5)
data$Diagnosis
sum(data$Diagnosis == as.numeric(model_ground_truth$fitted.values > 0.5))
sum(data$Diagnosis != as.numeric(model_ground_truth$fitted.values > 0.5))
sum(data$Diagnosis != as.numeric(model_majority$fitted.values > 0.5))
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
Diagnosis_majority <- colSums(Ann) / R
data_majority <- data[,-c(1, 11, 12, 13, 14)]
data_majority$DiagnosisMajority <- Diagnosis_majority
model_majority <- glm(DiagnosisMajority ~ ., family = binomial, data = data_majority)
as.numeric(model_majority$fitted.values > 0.5)
data$Diagnosis
sum(data$Diagnosis != as.numeric(model_ground_truth$fitted.values > 0.5))
sum(data$Diagnosis != as.numeric(model_majority$fitted.values > 0.5))
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
sum(data$Diagnosis != as.numeric(model_ground_truth$fitted.values > 0.5))
sum(data$Diagnosis != as.numeric(model_majority$fitted.values > 0.5))
source('E:/Studium/09_WS2122/SML_Project/data_screening.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/raykar_binary_classification.r', echo=TRUE)
source('E:/Studium/09_WS2122/SML_Project/logistic_regression_binary_classification.r', echo=TRUE)
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
R <- 3
#prepare data for ground truth logistic regression, fit logistic regression and display predictions
data_ground_truth <- data[,-c(1, 12, 13, 14)]
model_ground_truth <- glm(Diagnosis ~ ., family = binomial, data = data_ground_truth)
as.numeric(model_ground_truth$fitted.values > 0.5)
#prepare data for majority vote logistic regression, fit logistic regression and display predictions
Ann <- as.matrix(t(cbind(data$Diagnosis1,data$Diagnosis2,data$Diagnosis3)))
Diagnosis_majority <- colSums(Ann) / R
data_majority <- data[,-c(1, 11, 12, 13, 14)]
data_majority$DiagnosisMajority <- Diagnosis_majority
model_majority <- glm(DiagnosisMajority ~ ., family = binomial, data = data_majority)
as.numeric(model_majority$fitted.values > 0.5)
#eyeball misclassification error
sum(data$Diagnosis != as.numeric(model_ground_truth$fitted.values > 0.5))
sum(data$Diagnosis != as.numeric(model_majority$fitted.values > 0.5))
#load data, we work with R = 3 simulated erroneous annotators
data <- read.csv("Datasets/BreastCancerWisconsinAnnotated.csv")[,-1]
source('E:/Studium/09_WS2122/SML_Project/SML_LFC_Project/logistic_regression_binary_classification.r', echo=TRUE)
